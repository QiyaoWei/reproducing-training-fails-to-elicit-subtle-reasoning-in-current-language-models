#!/usr/bin/env python3
"""
Convert diamonds and function correctness datasets to VERL parquet format for GRPO training.
"""

import os
import pandas as pd
import numpy as np
import argparse
import logging
from datetime import datetime
from datasets import load_dataset
from tqdm import tqdm
from collections import defaultdict
from utils.prompts import SYSTEM_PROMPT_DIAMONDS, SYSTEM_PROMPT_FUNCTION_CORRECTNESS

def categorize_measurement_pattern(measurements):
    """Categorize measurement patterns for function correctness dataset rebalancing."""
    if not measurements:
        return "empty"
    
    true_count = sum(measurements)
    total_count = len(measurements)
    true_ratio = true_count / total_count
    
    if true_ratio == 1.0:
        return "all_true"
    elif true_ratio == 0.0:
        return "all_false"
    else: # 0 < true_ratio < 1
        return "mixed"

def rebalance_function_correctness_dataset(dataset, target_samples_per_category=1000, seed=42, logger=None):
    """Rebalance function correctness dataset to have uniform distribution of measurement patterns."""
    
    # Categorize all samples
    categories = defaultdict(list)
    for i, sample in enumerate(dataset):
        pattern = categorize_measurement_pattern(sample['measurements'])
        categories[pattern].append(i)
    
    # Log original distribution
    rebalance_message = "Original measurement pattern distribution:\n"
    for pattern, indices in categories.items():
        rebalance_message += f"  {pattern}: {len(indices)} samples\n"
    
    # Determine target size per category
    min_samples = min(len(indices) for indices in categories.values())
    target_per_category = min(target_samples_per_category, min_samples)
    
    rebalance_message += f"Rebalancing to {target_per_category} samples per category\n"
    
    if logger:
        log_and_print(logger, rebalance_message)
    else:
        print(rebalance_message)
    
    # Sample uniformly from each category
    balanced_indices = []
    for pattern, indices in categories.items():
        np.random.seed(seed)
        selected_indices = np.random.choice(indices, target_per_category, replace=False)
        balanced_indices.extend(selected_indices)
    
    # Create balanced dataset
    balanced_dataset = dataset.select(balanced_indices)
    
    # Log final distribution
    final_categories = defaultdict(list)
    for i, sample in enumerate(balanced_dataset):
        pattern = categorize_measurement_pattern(sample['measurements'])
        final_categories[pattern].append(i)
    
    final_message = "Balanced measurement pattern distribution:\n"
    for pattern, indices in final_categories.items():
        final_message += f"  {pattern}: {len(indices)} samples\n"
    
    if logger:
        log_and_print(logger, final_message)
    else:
        print(final_message)
    
    return balanced_dataset

def convert_to_verl_format(dataset, dataset_split="train", system_prompt=SYSTEM_PROMPT_DIAMONDS, dataset_name="diamonds-seed0"):
    """Convert diamonds dataset to VERL format."""
    
    data_rows = []
    
    for idx, example in enumerate(tqdm(dataset, desc=f"Processing {dataset_split}")):
        
        # Build the prompt in chat format
        # System prompt
        prompt = [{'role': 'system', 'content': system_prompt}]

        # User prompt
        if dataset_name.startswith("diamonds"):
            user_content = f"Now predict the measurements of the sensors for the following python snippet. Remember to think step by step through the code execution, then output your reasoning in <reasoning>...</reasoning> tags and the predicted measurements enclosed in <measurements>...</measurements> tags.\n{example['text']}\n"
        else: # function correctness
            user_content = f"Now predict the results of the test cases for the following Python function. Remember to think step by step through the code execution, then output your reasoning in <reasoning>...</reasoning> tags and the predicted results enclosed in <measurements>...</measurements> tags.\n{example['text']}\n"
        prompt.append({'role': 'user', 'content': user_content})
        
        # Convert measurements to string format for ground truth
        measurements = example.get('measurements', [])
        # Convert [True, False, True] to "[true, false, true]"
        measurements_str = str(measurements).lower().replace("'", "")
        
        # Create row in VERL format
        row = {
            'data_source': dataset_name,  # Custom data source identifier
            'prompt': prompt,
            'ability': 'reasoning',  # Task type
            'reward_model': {
                'ground_truth': measurements_str,  # Store as string
                'style': 'rule'  # Rule-based reward
            },
            'extra_info': {
                'index': idx,
                'is_correct': example.get('is_correct', False),  # latent variable
                'is_clean': example.get('is_clean', False),
                'difficulty': example.get('difficulty', -1), # default -1 for function correctness
                'measurements_list': measurements,  # Keep original list for reference
                'original_text': example['text']
            }
        }
        
        data_rows.append(row)
    
    # Convert to DataFrame
    df = pd.DataFrame(data_rows)
    
    return df

def setup_logging(output_dir, dataset_name):
    """Set up logging to both console and file."""
    # Create log filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"prepare_dataset_{dataset_name}_{timestamp}.log"
    log_path = os.path.join(output_dir, log_filename)
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path),
            logging.StreamHandler()  # This keeps console output
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Logging setup complete. Log file: {log_path}")
    return logger, log_path

def log_and_print(logger, message):
    """Log message and print to console."""
    logger.info(message)
    print(message)

def display_lv_statistics(train_df, val_df, test_df, logger=None):
    """Display the fraction of LV True and LV False (is_correct column) in each dataset split."""
    
    def get_lv_stats(df, split_name):
        """Get LV statistics for a single split."""
        is_correct_values = df['extra_info'].apply(lambda x: x['is_correct'])
        total = len(is_correct_values)
        true_count = sum(is_correct_values)
        false_count = total - true_count
        true_fraction = true_count / total if total > 0 else 0
        false_fraction = false_count / total if total > 0 else 0
        
        return {
            'total': total,
            'true_count': true_count,
            'false_count': false_count,
            'true_fraction': true_fraction,
            'false_fraction': false_fraction
        }
    
    stats_message = "\n" + "="*60 + "\n"
    stats_message += "LATENT VARIABLE (LV) STATISTICS - is_correct column\n"
    stats_message += "="*60 + "\n"
    
    splits = [("Train", train_df), ("Validation", val_df), ("Test", test_df)]
    
    for split_name, df in splits:
        stats = get_lv_stats(df, split_name)
        stats_message += f"\n{split_name} Set:\n"
        stats_message += f"  Total samples: {stats['total']}\n"
        stats_message += f"  LV True:  {stats['true_count']:4d} ({stats['true_fraction']:.3f})\n"
        stats_message += f"  LV False: {stats['false_count']:4d} ({stats['false_fraction']:.3f})\n"
    
    # Overall statistics
    total_samples = sum(get_lv_stats(df, name)['total'] for name, df in splits)
    total_true = sum(get_lv_stats(df, name)['true_count'] for name, df in splits)
    total_false = total_samples - total_true
    overall_true_fraction = total_true / total_samples if total_samples > 0 else 0
    overall_false_fraction = total_false / total_samples if total_samples > 0 else 0
    
    stats_message += f"\nOverall:\n"
    stats_message += f"  Total samples: {total_samples}\n"
    stats_message += f"  LV True:  {total_true:4d} ({overall_true_fraction:.3f})\n"
    stats_message += f"  LV False: {total_false:4d} ({overall_false_fraction:.3f})\n"
    stats_message += "="*60
    
    if logger:
        log_and_print(logger, stats_message)
    else:
        print(stats_message)


def create_balanced_by_lv(df, seed=42, logger=None):
    """Return a new DataFrame rebalanced to 50/50 by latent variable `extra_info['is_correct']`.

    Downsamples the majority class to the minority count. Returns None if a class has 0 samples.
    """
    # Extract LV booleans
    is_correct_series = df['extra_info'].apply(lambda x: x.get('is_correct', False))

    true_indices = is_correct_series[is_correct_series is True].index if isinstance(is_correct_series, pd.Series) else []
    false_indices = is_correct_series[is_correct_series is False].index if isinstance(is_correct_series, pd.Series) else []

    true_indices = is_correct_series[is_correct_series == True].index.tolist()
    false_indices = is_correct_series[is_correct_series == False].index.tolist()

    true_count = len(true_indices)
    false_count = len(false_indices)
    target_per_class = min(true_count, false_count)

    if target_per_class == 0:
        message = (
            "Cannot create balanced validation set: one class has 0 samples "
            f"(LV True: {true_count}, LV False: {false_count})."
        )
        if logger:
            log_and_print(logger, message)
        else:
            print(message)
        return None

    rng = np.random.default_rng(seed)
    selected_true = rng.choice(true_indices, size=target_per_class, replace=False)
    selected_false = rng.choice(false_indices, size=target_per_class, replace=False)

    selected_indices = np.concatenate([selected_true, selected_false])
    rng.shuffle(selected_indices)

    balanced_df = df.iloc[selected_indices].reset_index(drop=True)

    message = (
        "Created balanced validation set: "
        f"{len(balanced_df)} samples ({target_per_class} True / {target_per_class} False)"
    )
    if logger:
        log_and_print(logger, message)
    else:
        print(message)

    return balanced_df


def rebalance_dataset_by_lv(dataset, seed=42, logger=None):
    """Rebalance a HuggingFace Dataset to 50/50 by latent variable `is_correct`.

    Downsamples the majority class to the minority count, shuffles selected indices,
    and returns a new `datasets.Dataset` with the balanced rows.
    """
    # Collect indices by LV
    true_indices = []
    false_indices = []
    for i, sample in enumerate(dataset):
        is_correct = bool(sample.get('is_correct', False))
        if is_correct:
            true_indices.append(i)
        else:
            false_indices.append(i)

    true_count = len(true_indices)
    false_count = len(false_indices)
    target_per_class = min(true_count, false_count)

    if target_per_class == 0:
        message = (
            "Cannot rebalance split by LV: one class has 0 samples "
            f"(LV True: {true_count}, LV False: {false_count})."
        )
        if logger:
            log_and_print(logger, message)
        else:
            print(message)
        return dataset

    rng = np.random.default_rng(seed)
    selected_true = rng.choice(true_indices, size=target_per_class, replace=False)
    selected_false = rng.choice(false_indices, size=target_per_class, replace=False)

    selected_indices = np.concatenate([selected_true, selected_false])
    rng.shuffle(selected_indices)

    balanced_dataset = dataset.select(selected_indices.tolist())

    message = (
        "Rebalanced split to 50/50 LV: "
        f"{len(balanced_dataset)} samples ({target_per_class} True / {target_per_class} False)"
    )
    if logger:
        log_and_print(logger, message)
    else:
        print(message)

    return balanced_dataset

def main():
    """Main conversion function."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Convert dataset to VERL parquet format for GRPO training")
    parser.add_argument("--dataset-name", type=str, default="diamonds-seed0", 
                       choices=[f"diamonds-seed{i}" for i in range(8)] + ["function_correctness"],
                       help="Dataset to convert")
    parser.add_argument("--split-ratio", type=float, nargs=3, default=[0.8, 0.1, 0.1], 
                       help="Split ratio for train/val/test")
    parser.add_argument("--seed", type=int, default=42, 
                       help="Random seed for shuffling")
    parser.add_argument("--limit", type=lambda x: None if x.lower() == 'none' else int(x), default=None,
                        help="Limit total dataset size before splitting (default: None for all).")
    parser.add_argument("--output-dir", type=str, default=None,
                        help="Output directory for parquet files (default: ~/data/{dataset-name})")
    parser.add_argument("--limit-val-size", type=int, default=None,
                        help="Limit validation dataset size (default: None for no limit)")
    parser.add_argument("--save-val-balanced", action="store_true",
                        help="Also save val_balanced.parquet with 50/50 LV distribution by downsampling")
    parser.add_argument("--rebalance-by-lv", action="store_true",
                        help="Rebalance specified splits by latent variable 'is_correct' to 50/50")
    parser.add_argument("--rebalance-by-lv-splits", type=str, nargs="+", default=["val"],
                        choices=["train", "val", "test"],
                        help="Splits to apply LV rebalancing to (default: val)")
    
    args = parser.parse_args()
    
    # Validate ratios
    total_ratio = sum(args.split_ratio)
    if abs(total_ratio - 1.0) > 1e-6:
        raise ValueError(f"Train/val/test ratios must sum to 1.0, got {total_ratio}")
    
    # Create output directory
    if args.output_dir is None:
        # Use relative path for Docker compatibility - saves to current working directory
        output_dir = os.path.join("data", args.dataset_name)
    else:
        output_dir = os.path.expanduser(args.output_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    # Set up logging
    logger, log_path = setup_logging(output_dir, args.dataset_name)
    
    log_and_print(logger, f"Starting dataset preparation for {args.dataset_name}")
    log_and_print(logger, f"Output directory: {output_dir}")
    log_and_print(logger, f"Split ratios: {args.split_ratio}")
    log_and_print(logger, f"Random seed: {args.seed}")
    if args.limit:
        log_and_print(logger, f"Dataset limit: {args.limit}")
    
    log_and_print(logger, f"Loading {args.dataset_name} dataset...")
    
    # Load the full train split
    dataset = load_dataset(
        f"redwoodresearch/{args.dataset_name}",
        trust_remote_code=True,
        split="train"
    )
    
    log_and_print(logger, f"Total dataset size: {len(dataset)}")
    
    # Rebalance function correctness dataset
    if args.dataset_name == "function_correctness":
        log_and_print(logger, "Rebalancing function correctness dataset for uniform measurement pattern distribution...")
        dataset = rebalance_function_correctness_dataset(dataset, target_samples_per_category=int(len(dataset)/4), seed=args.seed, logger=logger)

    # Shuffle the dataset
    dataset = dataset.shuffle(seed=args.seed)
    
    # Apply limit if specified
    if args.limit:
        dataset = dataset.select(range(min(args.limit, len(dataset))))
        log_and_print(logger, f"Limited dataset size: {len(dataset)}")
    
    # Split into train/val/test
    total_size = len(dataset)
    train_size = int(total_size * args.split_ratio[0])
    val_size = int(total_size * args.split_ratio[1])
    test_size = total_size - train_size - val_size
    
    train_dataset = dataset.select(range(train_size))
    val_dataset = dataset.select(range(train_size, train_size + val_size))
    test_dataset = dataset.select(range(train_size + val_size, total_size))
    
    # Optionally limit val dataset size
    if args.limit_val_size:
        val_dataset = val_dataset.select(range(min(args.limit_val_size, len(val_dataset))))
        log_and_print(logger, f"Limited validation dataset to {len(val_dataset)} samples")
    
    log_and_print(logger, f"Split sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")

    # Optionally rebalance by LV (is_correct) after splitting
    if args.rebalance_by_lv:
        def _rebalance_named_split(split_name, ds):
            log_and_print(logger, f"Rebalancing {split_name} split by LV (is_correct) to 50/50...")
            return rebalance_dataset_by_lv(ds, seed=args.seed, logger=logger)

        if "train" in args.rebalance_by_lv_splits:
            train_dataset = _rebalance_named_split("train", train_dataset)
        if "val" in args.rebalance_by_lv_splits:
            val_dataset = _rebalance_named_split("validation", val_dataset)
        if "test" in args.rebalance_by_lv_splits:
            test_dataset = _rebalance_named_split("test", test_dataset)

    # Specify the system prompt for the dataset
    if args.dataset_name.startswith("diamonds"):
        system_prompt = SYSTEM_PROMPT_DIAMONDS
    elif args.dataset_name == "function_correctness":
        system_prompt = SYSTEM_PROMPT_FUNCTION_CORRECTNESS
    else:
        raise ValueError(f"Invalid dataset name: {args.dataset_name}")
    
    # Convert to VERL format
    log_and_print(logger, "\nConverting to VERL format...")
    train_df = convert_to_verl_format(train_dataset, "train", system_prompt, args.dataset_name)
    val_df = convert_to_verl_format(val_dataset, "validation", system_prompt, args.dataset_name)
    test_df = convert_to_verl_format(test_dataset, "test", system_prompt, args.dataset_name)
    
    # Display LV statistics
    display_lv_statistics(train_df, val_df, test_df, logger)
    
    # Save to parquet files
    log_and_print(logger, "\nSaving to parquet files...")
    train_path = os.path.join(output_dir, "train.parquet")
    val_path = os.path.join(output_dir, "val.parquet")
    test_path = os.path.join(output_dir, "test.parquet")
    
    train_df.to_parquet(train_path)
    log_and_print(logger, f"Saved {len(train_df)} training samples to {train_path}")
    
    val_df.to_parquet(val_path)
    log_and_print(logger, f"Saved {len(val_df)} validation samples to {val_path}")

    # Optionally save a balanced validation set (50/50 LV)
    if args.save_val_balanced:
        balanced_val_df = create_balanced_by_lv(val_df, seed=args.seed, logger=logger)
        if balanced_val_df is not None:
            val_balanced_path = os.path.join(output_dir, "val_balanced.parquet")
            balanced_val_df.to_parquet(val_balanced_path)
            log_and_print(logger, f"Saved {len(balanced_val_df)} balanced validation samples to {val_balanced_path}")
    
    test_df.to_parquet(test_path)
    log_and_print(logger, f"Saved {len(test_df)} test samples to {test_path}")
    
    log_and_print(logger, "\nDataset conversion complete!")
    
    # Verify files
    log_and_print(logger, "\nVerifying saved files...")
    for path in [train_path, val_path, test_path]:
        df_check = pd.read_parquet(path)
        log_and_print(logger, f"  {os.path.basename(path)}: {len(df_check)} rows, columns: {df_check.columns.tolist()}")
    
    log_and_print(logger, f"\nLog file saved to: {log_path}")


if __name__ == "__main__":
    main()